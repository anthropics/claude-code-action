{{- if .Values.workerCleanup.enabled }}
apiVersion: batch/v1
kind: CronJob
metadata:
  name: {{ include "peerbot.fullname" . }}-worker-cleanup
  namespace: {{ .Values.kubernetes.namespace }}
  labels:
    {{- include "peerbot.labels" . | nindent 4 }}
    app.kubernetes.io/component: worker-cleanup
spec:
  schedule: "*/30 * * * *"  # Every 30 minutes
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            {{- include "peerbot.selectorLabels" . | nindent 12 }}
            app.kubernetes.io/component: worker-cleanup
        spec:
          serviceAccountName: {{ include "peerbot.fullname" . }}-worker
          restartPolicy: OnFailure
          containers:
          - name: worker-cleanup
            image: "bitnami/kubectl:{{ .Values.workerCleanup.kubectlVersion | default "latest" }}"
            imagePullPolicy: Always
            command:
            - /bin/bash
            - -c
            - |
              set -e
              echo "Starting worker deployment cleanup..."
              
              # Get all worker deployments older than the threshold
              NAMESPACE="{{ .Values.kubernetes.namespace }}"
              MAX_AGE_MINUTES="{{ .Values.workerCleanup.maxAgeMinutes | default 60 }}"
              MAX_AGE_SECONDS=$((MAX_AGE_MINUTES * 60))
              CURRENT_TIME=$(date +%s)
              
              # Find deployments to clean up
              kubectl get deployments -n "$NAMESPACE" \
                -l "app.kubernetes.io/component=worker" \
                -o jsonpath='{range .items[*]}{.metadata.name}{" "}{.metadata.creationTimestamp}{"\n"}{end}' | \
              while read -r deployment_name creation_time; do
                if [ -n "$deployment_name" ] && [ -n "$creation_time" ]; then
                  # Convert creation time to epoch
                  creation_epoch=$(date -d "$creation_time" +%s 2>/dev/null || echo "0")
                  age_seconds=$((CURRENT_TIME - creation_epoch))
                  
                  if [ $age_seconds -gt $MAX_AGE_SECONDS ]; then
                    echo "Cleaning up deployment: $deployment_name (age: ${age_seconds}s > ${MAX_AGE_SECONDS}s)"
                    
                    # Scale down to 0 first for graceful cleanup
                    kubectl scale deployment "$deployment_name" -n "$NAMESPACE" --replicas=0 || true
                    
                    # Wait a moment for graceful shutdown
                    sleep 10
                    
                    # Delete the deployment
                    kubectl delete deployment "$deployment_name" -n "$NAMESPACE" --ignore-not-found=true
                    
                    echo "Deleted deployment: $deployment_name"
                  else
                    echo "Keeping deployment: $deployment_name (age: ${age_seconds}s < ${MAX_AGE_SECONDS}s)"
                  fi
                fi
              done
              
              # Clean up orphaned worker PVCs if configured
              {{- if .Values.workerCleanup.cleanupPVCs }}
              echo "Cleaning up orphaned worker PVCs..."
              kubectl get pvc -n "$NAMESPACE" \
                -l "app.kubernetes.io/component=worker" \
                -o jsonpath='{range .items[*]}{.metadata.name}{" "}{.metadata.creationTimestamp}{"\n"}{end}' | \
              while read -r pvc_name creation_time; do
                if [ -n "$pvc_name" ] && [ -n "$creation_time" ]; then
                  creation_epoch=$(date -d "$creation_time" +%s 2>/dev/null || echo "0")
                  age_seconds=$((CURRENT_TIME - creation_epoch))
                  
                  if [ $age_seconds -gt $MAX_AGE_SECONDS ]; then
                    # Check if PVC is still bound to a pod
                    if ! kubectl get pods -n "$NAMESPACE" -o jsonpath='{.items[*].spec.volumes[*].persistentVolumeClaim.claimName}' | grep -q "$pvc_name"; then
                      echo "Cleaning up orphaned PVC: $pvc_name"
                      kubectl delete pvc "$pvc_name" -n "$NAMESPACE" --ignore-not-found=true
                    fi
                  fi
                fi
              done
              {{- end }}
              
              echo "Worker cleanup completed successfully"
            resources:
              requests:
                cpu: "50m"
                memory: "128Mi"
              limits:
                cpu: "100m"
                memory: "256Mi"
  successfulJobsHistoryLimit: {{ .Values.workerCleanup.successfulJobsHistoryLimit | default 3 }}
  failedJobsHistoryLimit: {{ .Values.workerCleanup.failedJobsHistoryLimit | default 1 }}
{{- end }}